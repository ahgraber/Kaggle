for (i in 1:length(threshold)) {
classify <- ifelse(pred3 > threshold[i], 1, 0)
class3 <- append(class3, accuracy(trainOmit$Survived, classify))
}
names(class2) <- threshold
names(class3) <- threshold
class2
class3
# let's turn the probability into a hard classification
# borrowing/adapting function from class (Built by Matthew J. Schneider, Drexel LeBow):
accuracy <- function(actuals, classifications){
df <- data.frame(actuals, classifications);
TP <- nrow(df[df$classifications==1 & df$actuals==1,]);  # true positive
FP <- nrow(df[df$classifications==1 & df$actuals==0,]);  # false positive
FN <- nrow(df[df$classifications==0 & df$actuals==1,]);  # false negative
TN <- nrow(df[df$classifications==0 & df$actuals==0,]);  # true negative
recall=round(TP/(TP+FN),3)
precision=round(TP/(TP+FP),3)
accuracy=round((TP+TN)/(TP+FN+FP+TN),3)
tpr=recall
fpr=round(FP/(FP+TN),3)
fmeasure=round(2*precision*recall/(precision+recall),3)
scores=c(recall,precision,accuracy,tpr,fpr,fmeasure,TP,FP,FP,FN)
names(scores)=c("recall","precision","accuracy","tpr","fpr","fmeasure","TP","TN","FP","FN")
# tpr <- round(TP/(TP+FN),3)
# fpr <- round(FP/(FP+TN),3)
# scores=c(tpr,fpr,TP,FP,FP,FN)
# names(scores)=c("TruePos Rate","FalsePos Rate","TruePos","TrueNeg","FalsePos","FalseNeg")
# scoresB=c(TP,FP,FP,FN)
# names(scoresB)=c("TP","TN","FP","FN")
# print(scoresB)
return(list(scores));
}
# what probabilities from above model do we want to use as classification thresholds?
threshold <- c(.05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95)
class2 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred2 > threshold[i], 1, 0)
class2 <- append(class2, accuracy(trainOmit$Survived, classify))
}
class3 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred3 > threshold[i], 1, 0)
class3 <- append(class3, accuracy(trainOmit$Survived, classify))
}
names(class2) <- threshold
names(class3) <- threshold
class2
accuracy <- function(actuals, classifications){
# what probabilities from above model do we want to use as classification thresholds?
threshold <- c(.05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95)
class2 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred2 > threshold[i], 1, 0)
class2 <- append(class2, accuracy(trainOmit$Survived, classify))
}
class3 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred3 > threshold[i], 1, 0)
class3 <- append(class3, accuracy(trainOmit$Survived, classify))
}
names(class2) <- threshold
names(class3) <- threshold
class2
accuracy <- function(actuals, classifications){
# let's turn the probability into a hard classification
# borrowing/adapting function from class (Built by Matthew J. Schneider, Drexel LeBow):
accuracy <- function(actuals, classifications){
df <- data.frame(actuals, classifications);
TP <- nrow(df[df$classifications==1 & df$actuals==1,]);  # true positive
FP <- nrow(df[df$classifications==1 & df$actuals==0,]);  # false positive
FN <- nrow(df[df$classifications==0 & df$actuals==1,]);  # false negative
TN <- nrow(df[df$classifications==0 & df$actuals==0,]);  # true negative
precision <- round(TP/(TP+FP),3)  # positive hit rate
accuracy <- round((TP+TN)/(TP+FN+FP+TN),3)  # overall accuracy
tpr <- round(TP/(TP+FN),3)
fpr <- round(FP/(FP+TN),3)
scores <- c(recall,precision,accuracy,tpr,fpr,TP,FP,FP,FN)
names(scores) <- c("precision","accuracy","tpr","fpr","TP","TN","FP","FN")
# scoresB=c(TP,FP,FP,FN)
# names(scoresB)=c("TP","TN","FP","FN")
# print(scoresB)
return(list(scores));
}
}
)
# let's turn the probability into a hard classification
# borrowing/adapting function from class (Built by Matthew J. Schneider, Drexel LeBow):
accuracy <- function(actuals, classifications){
df <- data.frame(actuals, classifications);
TP <- nrow(df[df$classifications==1 & df$actuals==1,]);  # true positive
FP <- nrow(df[df$classifications==1 & df$actuals==0,]);  # false positive
FN <- nrow(df[df$classifications==0 & df$actuals==1,]);  # false negative
TN <- nrow(df[df$classifications==0 & df$actuals==0,]);  # true negative
precision <- round(TP/(TP+FP),3)  # positive hit rate
accuracy <- round((TP+TN)/(TP+FN+FP+TN),3)  # overall accuracy
tpr <- round(TP/(TP+FN),3)
fpr <- round(FP/(FP+TN),3)
scores <- c(recall,precision,accuracy,tpr,fpr,TP,FP,FP,FN)
names(scores) <- c("precision","accuracy","tpr","fpr","TP","TN","FP","FN")
# scoresB=c(TP,FP,FP,FN)
# names(scoresB)=c("TP","TN","FP","FN")
# print(scoresB)
return(list(scores));
}
# what probabilities from above model do we want to use as classification thresholds?
threshold <- c(.05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95)
class2 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred2 > threshold[i], 1, 0)
class2 <- append(class2, accuracy(trainOmit$Survived, classify))
}
# let's turn the probability into a hard classification
# borrowing/adapting function from class (Built by Matthew J. Schneider, Drexel LeBow):
accuracy <- function(actuals, classifications){
df <- data.frame(actuals, classifications);
TP <- nrow(df[df$classifications==1 & df$actuals==1,]);  # true positive
FP <- nrow(df[df$classifications==1 & df$actuals==0,]);  # false positive
FN <- nrow(df[df$classifications==0 & df$actuals==1,]);  # false negative
TN <- nrow(df[df$classifications==0 & df$actuals==0,]);  # true negative
precision <- round(TP/(TP+FP),3)  # positive hit rate
accuracy <- round((TP+TN)/(TP+FN+FP+TN),3)  # overall accuracy
tpr <- round(TP/(TP+FN),3)
fpr <- round(FP/(FP+TN),3)
scores <- c(precision,accuracy,tpr,fpr,TP,FP,FP,FN)
names(scores) <- c("precision","accuracy","tpr","fpr","TP","TN","FP","FN")
# scoresB=c(TP,FP,FP,FN)
# names(scoresB)=c("TP","TN","FP","FN")
# print(scoresB)
return(list(scores));
}
# what probabilities from above model do we want to use as classification thresholds?
threshold <- c(.05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95)
class2 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred2 > threshold[i], 1, 0)
class2 <- append(class2, accuracy(trainOmit$Survived, classify))
}
class3 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred3 > threshold[i], 1, 0)
class3 <- append(class3, accuracy(trainOmit$Survived, classify))
}
names(class2) <- threshold
names(class3) <- threshold
class2
class3
# what probabilities from above model do we want to use as classification thresholds?
# threshold <- c(.05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95)
threshold <- c(.5, .55, .57, .6, .62, .65)
class2 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred2 > threshold[i], 1, 0)
class2 <- append(class2, accuracy(trainOmit$Survived, classify))
}
class3 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred3 > threshold[i], 1, 0)
class3 <- append(class3, accuracy(trainOmit$Survived, classify))
}
names(class2) <- threshold
names(class3) <- threshold
class2
class3
# what probabilities from above model do we want to use as classification thresholds?
# threshold <- c(.05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95)
threshold <- c(.58, .59, .6, .61, .62)
class2 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred2 > threshold[i], 1, 0)
class2 <- append(class2, accuracy(trainOmit$Survived, classify))
}
class3 <- list()
for (i in 1:length(threshold)) {
classify <- ifelse(pred3 > threshold[i], 1, 0)
class3 <- append(class3, accuracy(trainOmit$Survived, classify))
}
names(class2) <- threshold
names(class3) <- threshold
class2
class3
# first we have to set the testing data up the same way we did before
temp <- model.matrix(~ -1 + Embarked, data=train)
test <- test %>%
select(-Embarked) %>%  # remove original factor variables
cbind(temp) %>%  # add new binary variables to the data set
select(-Embarked)  # remove Embarked column which has 1's for missing embarked locations
# first we have to set the testing data up the same way we did before
temp <- model.matrix(~ -1 + Embarked, data=test)
test <- test %>%
select(-Embarked) %>%  # remove original factor variables
cbind(temp) %>%  # add new binary variables to the data set
select(-Embarked)  # remove Embarked column which has 1's for missing embarked locations
View(test)
test <- read.csv("test.csv")
# first we have to set the testing data up the same way we did before
test <- read.csv("test.csv")
# first we have to set the testing data up the same way we did before
test <- read.csv("test.csv")
rownames(test) <- test$PassengerId
test <- test %>%
select(-PassengerId, -Name, -Ticket, -Cabin) # Name, Ticket# shouldn't impact; Cabin has missing
temp <- model.matrix(~ -1 + Embarked, data=test)
test <- test %>%
select(-Embarked) %>%  # remove original factor variables
cbind(temp) %>%  # add new binary variables to the data set
select(-Embarked)  # remove Embarked column which has 1's for missing embarked locations
rm(temp)
View(test)
test <- test %>%
select(-Embarked) %>%  # remove original factor variables
cbind(temp)  # add new binary variables to the data set
temp <- model.matrix(~ -1 + Embarked, data=test)
test <- test %>%
select(-Embarked) %>%  # remove original factor variables
cbind(temp)  # add new binary variables to the data set
View(test)
View(train)
# even though we went with the NA.exclude data for train, we have to fill in data for test
# or we won't get predictions for all rows
# install.packages("VIM")
library(VIM)
test <- kNN(test) %>%
select(1:9)   # drops additional columns that kNN creates to identify cells to fill
prediction <- predict(pred3, newdata = test, type = "response")
rm(temp)
prediction <- predict(pred3, newdata = test, type = "response")
prediction <- predict(logitOmit3, newdata = test, type = "response")
# first we have to set the testing data up the same way we did before
test <- read.csv("test.csv")
rownames(test) <- test$PassengerId
test <- test %>%
select(-Name, -Ticket, -Cabin) # Name, Ticket# shouldn't impact; Cabin has missing
temp <- model.matrix(~ -1 + Embarked, data=test)
test <- test %>%
select(-Embarked) %>%  # remove original factor variables
cbind(temp)  # add new binary variables to the data set
rm(temp)
# even though we went with the NA.exclude data for train, we have to fill in data for test
# or we won't get predictions for all rows
# install.packages("VIM")
library(VIM)
test <- kNN(test) %>%
select(1:9)   # drops additional columns that kNN creates to identify cells to fill
View(test)
# first we have to set the testing data up the same way we did before
test <- read.csv("test.csv")
rownames(test) <- test$PassengerId
test <- test %>%
select(-Name, -Ticket, -Cabin) # Name, Ticket# shouldn't impact; Cabin has missing
temp <- model.matrix(~ -1 + Embarked, data=test)
test <- test %>%
select(-Embarked) %>%  # remove original factor variables
cbind(temp)  # add new binary variables to the data set
rm(temp)
# even though we went with the NA.exclude data for train, we have to fill in data for test
# or we won't get predictions for all rows
# install.packages("VIM")
library(VIM)
test <- kNN(test) %>%
select(1:10)   # drops additional columns that kNN creates to identify cells to fill
prediction <- predict(logitOmit3, newdata = test, type = "response")
# use the same classificiation threshold (.6) that we used in the verification above
classify <- ifelse(prediction > 0.60, 1, 0)
out <- test %>%
select(PassengerId) %>%
cbind(classify)
write.csv(out, "TitanicLogicstic.csv", row.names=F)
# use the same classificiation threshold (.6) that we used in the verification above
survived <- ifelse(prediction > 0.60, 1, 0)
out <- test %>%
select(PassengerId) %>%
cbind(survived) %>%
write.csv(out, "TitanicLogicstic.csv", row.names=F)
out <- test %>%
select(PassengerId) %>%
cbind(survived) %>%
write.csv(out, "TitanicLogicstic.csv", row.names=F)
out <- test %>%
select(PassengerId) %>%
cbind(survived)
write.csv(out, "TitanicLogicstic.csv", row.names=F)
# install.packages("e1071")
library("e1071")
trainOmit <- na.exclude(train)
svm(survived ~ ., data = trainOmit, cost = 1, epsilon = 0.1)
svm(Survived ~ ., data = trainOmit, cost = 1, epsilon = 0.1)
svmOmit <- svm(Survived ~ ., data = trainOmit, cost = 1, epsilon = 0.1)
summary(svmOmit)
svmOmit$coef0
svmOmit$coefs
svmOmit$sv
svmOmit$SV
svmOmit$index
svmOmit$rho
svmOmit$sigma
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 10, epsilon = 0.1)
svmOmit2$SV
head(svmOmit2$SV)
head(svmOmit$SV)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 100, epsilon = 0.1)
head(svmOmit2$SV)
head(svmOmit$SV)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 1000, epsilon = 0.1)
head(svmOmit2$SV)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 1, epsilon = 1)
head(svmOmit2$SV)
head(svmOmit$SV)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 1, epsilon = 10)
head(svmOmit2$SV)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 1, epsilon = 0)
head(svmOmit2$SV)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 1, epsilon = .001)
head(svmOmit2$SV)
head(svmOmit$SV)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 1, epsilon = 1)
head(svmOmit2$SV)
head(svmOmit$SV)
svmOmit <- svm(Survived ~ ., data = trainOmit, cost = 10, epsilon = 0.1)
head(svmOmit$SV)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, cost = 1, epsilon = .05)
head(svmOmit2$SV)
predSVM <- predict(svmOmit, type="response")
predSVM2 <- predict(svmOmit2, type="response")
svmOmit <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 0.1)
head(svmOmit$SV)
predSVM <- predict(svmOmit, type="response")
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 1, epsilon = .05)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM))
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = .05)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM2)
class2  # most accurate at .61 threshold
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 100, epsilon = .05)
accuracy(trainOmit$Survived, predSVM2)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 1000, epsilon = .05)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = .01)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 0)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 1)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 10)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 1, epsilon = .1)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 0, epsilon = .1)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = -1, epsilon = .1)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 1, epsilon = .1)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = .01, epsilon = .1)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = .01, epsilon = 1000)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ Pclass + Sex + Age + SibSp, data = trainOmit, type = "C",
cost = .01, epsilon = 1000)
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 10)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 100)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 1000)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 10000)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10, epsilon = 0)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = .1, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 1, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 50, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 100, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 500, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 1000, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 10000, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 100000, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 1000000, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 100000, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 100000, epsilon = 0.1)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 100000, epsilon = -1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 100000, epsilon = 0.1)
predSVM <- predict(svmOmit, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ ., data = trainOmit, type = "C", cost = 100000, epsilon = 10)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ Pclass + Sex + Age + SibSp, data = trainOmit, type = "C",
cost = 100000, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ Pclass + Sex + Age + SibSp, data = trainOmit, type = "C",
cost = 10000, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
svmOmit2 <- svm(Survived ~ Pclass + Sex + Age + SibSp, data = trainOmit, type = "C",
cost = 50000, epsilon = 0.1)
svmOmit2 <- svm(Survived ~ Pclass + Sex + Age + SibSp, data = trainOmit, type = "C",
cost = 20000, epsilon = 0.1)
predSVM2 <- predict(svmOmit2, type="response")
accuracy(trainOmit$Survived, predSVM)
accuracy(trainOmit$Survived, predSVM2)
confint(svmOmit)
confint(predSVM)
# first we have to set the testing data up the same way we did before
test <- read.csv("test.csv")
rownames(test) <- test$PassengerId
test <- test %>%
select(-Name, -Ticket, -Cabin) # Name, Ticket# shouldn't impact; Cabin has missing
temp <- model.matrix(~ -1 + Embarked, data=test)
test <- test %>%
select(-Embarked) %>%  # remove original factor variables
cbind(temp)  # add new binary variables to the data set
rm(temp)
# even though we went with the NA.exclude data for train, we have to fill in data for test
# or we won't get predictions for all rows
# install.packages("VIM")
library(VIM)
test <- kNN(test) %>%
select(1:10)   # drops additional columns that kNN creates to identify cells to fill
# use model to identify probability
prediction <- predict(logitOmit3, newdata = test, type = "response")
# use the same classificiation threshold (.6) that we used in the verification above
survived <- ifelse(prediction > 0.60, 1, 0)
out <- test %>%
select(PassengerId) %>%
cbind(survived)
# save the output!
write.csv(out, "TitanicSVM.csv", row.names=F)
str(train$Fare)
summary(train$Fare)
summary(train$Age)
plot(trainOmit$Age, trainOmit$Survived)
plot(trainOmit$Age, trainOmit$Survived, type="h")
plot(trainOmit$Age, trainOmit$Survived, type="b")
plot(trainOmit$Age, trainOmit$Survived, type="o")
plot(trainOmit$Age, trainOmit$Survived, type="p")
smoothScatter(trainOmit$Age, trainOmit$Survived, type="p")
smoothScatter(trainOmit$Age, trainOmit$Survived)
install.packages("randomForest")
trainScale <- scale(trainOmit)
trainOmit <- na.exclude(train)
trainScale <- scale(trainOmit)
# consider binning Age, Fare
hist(trainOmit$Age)
hist(trainOmit$Fare)
scaleAge <- scale(trainOmit$Age)
scaleFare <- scale(trainOmit$Fare)
